{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid activation function and its derivative \n",
    "soft max activation function for multi class output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialising weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, binary_output_size, class_output_size):\n",
    "    np.random.seed(42)\n",
    "    W1_binary = np.random.randn(input_size, hidden_size) * 0.01 \n",
    "    b1_binary = np.zeros((1, hidden_size))         \n",
    "    W1_class = np.random.randn(input_size, hidden_size) * 0.01 \n",
    "    b1_class = np.zeros((1, hidden_size))                            \n",
    "    W2_binary = np.random.randn(hidden_size, binary_output_size) * 0.01\n",
    "    b2_binary = np.zeros((1, binary_output_size))\n",
    "    W2_class = np.random.randn(hidden_size, class_output_size) * 0.01\n",
    "    b2_class = np.zeros((1, class_output_size))\n",
    "    return W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining forward and backward propagation and computing the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class):\n",
    "    Z1_binary = np.dot(X, W1_binary) + b1_binary\n",
    "    A1_binary = sigmoid(Z1_binary)\n",
    "\n",
    "    Z1_class = np.dot(X,W1_class)+b1_class\n",
    "    A1_class = sigmoid(Z1_class)\n",
    "\n",
    "\n",
    "    Z2_binary = np.dot(A1_binary, W2_binary) + b2_binary\n",
    "    A2_binary = sigmoid(Z2_binary)\n",
    "\n",
    "    Z2_class = np.dot(A1_class, W2_class) + b2_class\n",
    "    A2_class = softmax(Z2_class)\n",
    "\n",
    "    cache = {\"Z1_binary\": Z1_binary, \"A1_binary\":  A1_binary,\"Z1_class\": Z1_class,\"A1_class\":A1_class, \"Z2_binary\": Z2_binary, \"A2_binary\": A2_binary, \"Z2_class\": Z2_class, \"A2_class\": A2_class}\n",
    "    return A2_binary, A2_class, cache\n",
    "\n",
    "\n",
    "def compute_binary_loss(y_binary, A2_binary):\n",
    "    m = y_binary.shape[0]\n",
    "    loss = -np.mean(y_binary * np.log(A2_binary) + (1 - y_binary) * np.log(1 - A2_binary))\n",
    "    return loss\n",
    "\n",
    "def compute_class_loss(y_class, A2_class):\n",
    "    m = y_class.shape[0]\n",
    "    loss = -np.mean(np.sum(y_class * np.log(A2_class), axis=1))\n",
    "    return loss\n",
    "\n",
    "def backward_propagation(X, y_binary, y_class, cache, W2_binary, W2_class):\n",
    "    m = X.shape[0]\n",
    "    A1_binary = cache[\"A1_binary\"]\n",
    "    A1_class = cache[\"A1_class\"]\n",
    "    A2_binary = cache[\"A2_binary\"]\n",
    "    A2_class = cache[\"A2_class\"]\n",
    "\n",
    "    dZ2_binary = A2_binary - y_binary\n",
    "    dW2_binary = np.dot(A1_binary.T, dZ2_binary) / m\n",
    "    db2_binary = np.sum(dZ2_binary, axis=0, keepdims=True) / m\n",
    "\n",
    "    dZ2_class = A2_class - y_class\n",
    "    dW2_class = np.dot(A1_class.T, dZ2_class) / m\n",
    "    db2_class = np.sum(dZ2_class, axis=0, keepdims=True) / m\n",
    "\n",
    "    dZ1_binary = np.dot(dZ2_binary, W2_binary.T)\n",
    "    dZ1_class = np.dot(dZ2_class, W2_class.T)\n",
    "    \n",
    "    dZ1_binary *= sigmoid_derivative(cache[\"Z1_binary\"])\n",
    "    dZ1_class *= sigmoid_derivative(cache[\"Z1_class\"])\n",
    "    \n",
    "    dW1_binary = np.dot(X.T, dZ1_binary) / m\n",
    "    db1_binary = np.sum(dZ1_binary, axis=0, keepdims=True) / m\n",
    "    \n",
    "    dW1_class = np.dot(X.T, dZ1_class) / m\n",
    "    db1_class = np.sum(dZ1_class, axis=0, keepdims=True) / m\n",
    "\n",
    "    gradients = {\n",
    "        \"dW1_binary\": dW1_binary,\n",
    "        \"db1_binary\": db1_binary,\n",
    "        \"dW1_class\": dW1_class,\n",
    "        \"db1_class\": db1_class,\n",
    "        \"dW2_binary\": dW2_binary,\n",
    "        \"db2_binary\": db2_binary,\n",
    "        \"dW2_class\": dW2_class,\n",
    "        \"db2_class\": db2_class\n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updating the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class, gradients, learning_rate):\n",
    "\n",
    "    W1_binary -= learning_rate * gradients[\"dW1_binary\"]\n",
    "    b1_binary -= learning_rate * gradients[\"db1_binary\"]\n",
    "    W1_class -= learning_rate * gradients[\"dW1_class\"]\n",
    "    b1_class -= learning_rate * gradients[\"db1_class\"]\n",
    "    W2_binary -= learning_rate * gradients[\"dW2_binary\"]\n",
    "    b2_binary -= learning_rate * gradients[\"db2_binary\"]\n",
    "    W2_class -= learning_rate * gradients[\"dW2_class\"]\n",
    "    b2_class -= learning_rate * gradients[\"db2_class\"]\n",
    "    return W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining to train the dats set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, y_binary, y_class, hidden_size=64, learning_rate=0.01, epochs=100):\n",
    "    input_size = X.shape[1]\n",
    "    binary_output_size = 1\n",
    "    class_output_size = y_class.shape[1]\n",
    "\n",
    "    W1_binary, b1_binary, W1_class,b1_class,W2_binary, b2_binary, W2_class, b2_class = initialize_parameters(input_size, hidden_size, binary_output_size, class_output_size)\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        A2_binary, A2_class, cache = forward_propagation(X, W1_binary, b1_binary, W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class)\n",
    "\n",
    "        binary_loss = compute_binary_loss(y_binary, A2_binary)\n",
    "        class_loss = compute_class_loss(y_class, A2_class)\n",
    "        total_loss = binary_loss + class_loss\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        gradients = backward_propagation(X, y_binary, y_class, cache, W2_binary, W2_class)\n",
    "\n",
    "        W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class = update_parameters(W1_binary, b1_binary,W1_class , b1_class, W2_binary, b2_binary, W2_class, b2_class, gradients, learning_rate)\n",
    "\n",
    "                \n",
    "    \n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.title('Cost Convergence')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    return W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting the testing data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1_binary, b1_binary, W1_class,b1_class,W2_binary, b2_binary, W2_class, b2_class):\n",
    "    A2_binary, A2_class, _ = forward_propagation(X, W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class)\n",
    "    binary_predictions = (A2_binary > 0.5).astype(int)\n",
    "    class_predictions = np.argmax(A2_class, axis=1)\n",
    "    return binary_predictions, class_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =  r\"C:\\Users\\91944\\OneDrive - Indian Institute of Technology Indian School of Mines Dhanbad\\Desktop\\ml\\nn_train.csv\"  \n",
    "data = pd.read_csv(file_path).drop(columns=['ID'])\n",
    "\n",
    "X = data.iloc[:, :-2].values \n",
    "y_binary = data.iloc[:, -2].values.reshape(-1, 1)  \n",
    "y_class = pd.get_dummies(data.iloc[:, -1]).values  \n",
    "\n",
    "    \n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "train_size = int(0.8 * len(X))\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "\n",
    "X_train_mean=X_train.mean()\n",
    "X_train_std =X_train.std()                       #normalising the data       \n",
    "X_train=(X_train-X_train_mean)/X_train_std\n",
    "X_test = (X_test-X_train_mean)/X_train_std\n",
    "\n",
    "y_binary_train, y_binary_test = y_binary[train_indices], y_binary[test_indices]\n",
    "y_class_train, y_class_test = y_class[train_indices], y_class[test_indices]\n",
    "\n",
    "W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class = train_neural_network(X_train, y_binary_train, y_class_train, hidden_size=128, learning_rate=0.8, epochs=7)\n",
    "\n",
    "binary_predictions, class_predictions = predict(X_test, W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class)\n",
    "\n",
    "binary_accuracy = np.mean(binary_predictions == y_binary_test)\n",
    "class_predictions_one_hot = np.zeros_like(y_class_test)\n",
    "class_predictions_one_hot[np.arange(len(class_predictions)), class_predictions] = 1\n",
    "class_accuracy = np.mean(np.all(class_predictions_one_hot == y_class_test, axis=1))\n",
    "\n",
    "binary_f1 = f1_score(y_binary_test.flatten(), binary_predictions.flatten())\n",
    "\n",
    "print(f\"Binary Accuracy: {binary_accuracy:.4f}\")\n",
    "print(f\"Binary F1 Score: {binary_f1:.4f}\")\n",
    "print(f\"Class Accuracy: {class_accuracy:.4f}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(r\"C:\\Users\\91944\\OneDrive - Indian Institute of Technology Indian School of Mines Dhanbad\\Desktop\\ml\\nn_test.csv\")\n",
    "\n",
    "X_new = test_data.drop(columns=[\"ID\"]).values\n",
    "\n",
    "X_new = (X_new - X_train_mean)/X_train_std \n",
    "\n",
    "binary_new_pred, class_new_pred = predict(X_new, W1_binary, b1_binary,W1_class,b1_class, W2_binary, b2_binary, W2_class, b2_class)\n",
    "\n",
    "\n",
    "test_data['binary_Predicted'] = binary_new_pred \n",
    "test_data['class_predicted'] =  class_new_pred\n",
    "\n",
    "\n",
    "test_data.to_csv(r\"C:\\Users\\91944\\OneDrive - Indian Institute of Technology Indian School of Mines Dhanbad\\Desktop\\ml\\nn_test_predicted.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved in the original test data file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
