{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt                                                          \n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining sigmoid function , cost function and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(y, y_predicted):\n",
    "    m = len(y)\n",
    "    cost= -(1 / m) * np.sum(y * np.log(y_predicted) + (1 - y) * np.log(1 - y_predicted))\n",
    "    return cost\n",
    "\n",
    "def train_logistic_regression(X, y, learning_rate=0.01, epochs=1000):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        z = np.dot(X, weights) + bias\n",
    "        y_predicted = sigmoid(z)\n",
    "\n",
    "        \n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "        \n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "        \n",
    "        loss = compute_loss(y, y_predicted)\n",
    "        loss_history.append(loss)\n",
    "    \n",
    "\n",
    "    return weights, bias, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    linear_model = np.dot(X, weights) + bias\n",
    "    y_predicted = sigmoid(linear_model)\n",
    "    return np.array([1 if i > 0.5 else 0 for i in y_predicted])   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0  \n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the dats set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\91944\\OneDrive - Indian Institute of Technology Indian School of Mines Dhanbad\\Desktop\\ml\\binary_classification_train.csv\")\n",
    "data = data.drop(columns=['ID'])\n",
    "\n",
    "\n",
    "X = data.drop(columns=['Class']).values\n",
    "y = data['Class'].values\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)  # Normalize features\n",
    "\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "X_train_mean = X_train.mean()\n",
    "X_train_std = X_train.std()\n",
    "\n",
    "X_train_ = (X_train - X_train_mean)/X_train_std\n",
    "X_test_  = (X_test - X_train_mean)/X_train_std\n",
    "\n",
    "weights, bias, loss_history = train_logistic_regression(X_train_, y_train, learning_rate=0.1, epochs=500)\n",
    "print(\"loss function = \" , loss_history[-1])\n",
    "\n",
    "y_pred = predict(X_train, weights, bias)\n",
    "print(\"Class distribution in y_train:\", np.bincount(y_train))\n",
    "print(\"Class distribution in y_pred:\", np.bincount(y_pred))\n",
    "\n",
    "\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "metric = f1_score(y_train,y_pred)\n",
    "print(\"f1_score for testing data set is \",metric)\n",
    "\n",
    "print(f\"Accuracy on training dataset: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(r\"C:\\Users\\91944\\OneDrive - Indian Institute of Technology Indian School of Mines Dhanbad\\Desktop\\ml\\binary_classification_test.csv\")\n",
    "\n",
    "\n",
    "X_new = test_data.drop(columns=[\"ID\"]).values\n",
    " \n",
    "\n",
    "X_new = (X_new - X_train_mean)/X_train_std \n",
    "\n",
    "y_new_pred = predict(X_new, weights, bias)\n",
    "\n",
    "\n",
    "test_data['Predicted'] = y_new_pred\n",
    "test_data.to_csv(r\"C:\\Users\\91944\\OneDrive - Indian Institute of Technology Indian School of Mines Dhanbad\\Desktop\\ml\\binary_classification_test_pred.csv\", index=False)\n",
    "print(\"Predictions saved in the original test data file.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
